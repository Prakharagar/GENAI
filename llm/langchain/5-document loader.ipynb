{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv loader\n",
    "#json/Yaml Loader\n",
    "#pdf loader\n",
    "#Sql loader\n",
    "#python df loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/sample.csv', 'row': 0}, page_content='id: 1\\ntext: i love machine learning'), Document(metadata={'source': '../data/sample.csv', 'row': 1}, page_content='id: 2\\ntext: India is best place to visit'), Document(metadata={'source': '../data/sample.csv', 'row': 2}, page_content='id: 3\\ntext: LLM makes our work easy')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "csv_loader=CSVLoader(file_path='../data/sample.csv',\n",
    "                  )\n",
    "csv_doc=csv_loader.load()\n",
    "print(csv_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: i love machine learning\n",
      "text: India is best place to visit\n",
      "text: LLM makes our work easy\n"
     ]
    }
   ],
   "source": [
    "csv_loader=CSVLoader(file_path='../data/sample.csv',\n",
    "                     csv_args={\n",
    "                                \"delimiter\": \",\",\n",
    "                               },\n",
    "                  )\n",
    "csv_doc=csv_loader.load()\n",
    "#print(csv_doc)\n",
    "for doc in csv_doc:\n",
    "    print(doc.page_content.split(\"\\n\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='1', metadata={'source': 'tweet'}, page_content='I had chocalate chip pancakes and scrambled eggs for breakfast this morning.'),\n",
       " Document(id='2', metadata={'source': 'news'}, page_content='The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.'),\n",
       " Document(id='3', metadata={'source': 'tweet'}, page_content='Building an exciting new project with LangChain - come check it out!'),\n",
       " Document(id='4', metadata={'source': 'news'}, page_content='Robbers broke into the city bank and stole $1 million in cash.'),\n",
       " Document(id='5', metadata={'source': 'tweet'}, page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\"),\n",
       " Document(id='6', metadata={'source': 'website'}, page_content='Is the new iPhone worth the price? Read this review to find out.'),\n",
       " Document(id='7', metadata={'source': 'website'}, page_content='The top 10 soccer players in the world right now.'),\n",
       " Document(id='8', metadata={'source': 'tweet'}, page_content='LangGraph is the best framework for building stateful, agentic applications!'),\n",
       " Document(id='9', metadata={'source': 'news'}, page_content='The stock market is down 500 points today due to fears of a recession.'),\n",
       " Document(id='10', metadata={'source': 'tweet'}, page_content='I have a bad feeling I am going to get deleted :(')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "document_1 = Document(\n",
    "    page_content=\"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=1,\n",
    ")\n",
    "\n",
    "document_2 = Document(\n",
    "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=2,\n",
    ")\n",
    "\n",
    "document_3 = Document(\n",
    "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=3,\n",
    ")\n",
    "\n",
    "document_4 = Document(\n",
    "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=4,\n",
    ")\n",
    "\n",
    "document_5 = Document(\n",
    "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=5,\n",
    ")\n",
    "\n",
    "document_6 = Document(\n",
    "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    "    id=6,\n",
    ")\n",
    "\n",
    "document_7 = Document(\n",
    "    page_content=\"The top 10 soccer players in the world right now.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    "    id=7,\n",
    ")\n",
    "\n",
    "document_8 = Document(\n",
    "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=8,\n",
    ")\n",
    "\n",
    "document_9 = Document(\n",
    "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=9,\n",
    ")\n",
    "\n",
    "document_10 = Document(\n",
    "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=10,\n",
    ")\n",
    "\n",
    "documents = [\n",
    "    document_1,\n",
    "    document_2,\n",
    "    document_3,\n",
    "    document_4,\n",
    "    document_5,\n",
    "    document_6,\n",
    "    document_7,\n",
    "    document_8,\n",
    "    document_9,\n",
    "    document_10,\n",
    "]\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to create a custom Document Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='../data/sample.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'line_number': 0, 'source': '../data/sample.csv'}, page_content=' i love machine learning'), Document(metadata={'line_number': 1, 'source': '../data/sample.csv'}, page_content=' India is best place to visit'), Document(metadata={'line_number': 2, 'source': '../data/sample.csv'}, page_content=' LLM makes our work easy')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Omen\\AppData\\Local\\Temp\\ipykernel_9728\\3470329385.py:8: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  page_content=row[1],\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(file_path)\n",
    "document=[]\n",
    "for index, row in df.iterrows():\n",
    "    #print(row[1]) \n",
    "    document.append(\n",
    "            Document(\n",
    "            page_content=row[1],\n",
    "            metadata={\"line_number\": index, \"source\": file_path},\n",
    "                )) \n",
    "print(document) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'line_number': 0, 'source': '../data/sample.csv'}, page_content='id,text\\n'), Document(metadata={'line_number': 1, 'source': '../data/sample.csv'}, page_content='1, i love machine learning\\n'), Document(metadata={'line_number': 2, 'source': '../data/sample.csv'}, page_content='2, India is best place to visit\\n'), Document(metadata={'line_number': 3, 'source': '../data/sample.csv'}, page_content='3, LLM makes our work easy')]\n"
     ]
    }
   ],
   "source": [
    "with open(file_path, encoding=\"utf-8\") as f:\n",
    "    line_number = 0\n",
    "    document=[]\n",
    "    for line in f:\n",
    "        document.append(\n",
    "            Document(\n",
    "            page_content=line,\n",
    "            metadata={\"line_number\": line_number, \"source\": file_path},\n",
    "                ))\n",
    "        line_number += 1\n",
    "    \n",
    "print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw JSON Data: {'messages': [{'content': 'Message 1'}, {'content': 'Message 2'}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load and inspect the raw JSON file manually\n",
    "with open('../data/config.json', 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "print(\"Raw JSON Data:\", json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'D:\\\\projects\\\\GENAI\\\\llm\\\\data\\\\config.json', 'seq_num': 1}, page_content='Message 1'), Document(metadata={'source': 'D:\\\\projects\\\\GENAI\\\\llm\\\\data\\\\config.json', 'seq_num': 2}, page_content='Message 2')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "loader = JSONLoader(\n",
    "    file_path='../data/config.json',\n",
    "    jq_schema='.messages[].content',\n",
    "    text_content=False)\n",
    "\n",
    "data = loader.load()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/guidlines.pdf', 'page': 0}, page_content=\"Scala Coding Guidelines  \\nQuantexa Coding Guidelines:  \\nGeneral \\nScala/Spark   \\nCamel Case  Use camel casing, not underscores! Vals/vars/methods(defs) should all \\nbe lowerCamelCase, types/objects should all be UpperCamelCase (also \\nknown as ProperCase), for further details please  see this style guide .  \\nExceptions  \\n1. In ETL processes, the raw data should be read into a parquet \\nmaintaining all of the source field names exactly.   \\n2. Constant values - use ProperCase. For example, val YearInDays = \\n365 \\n3. When using acronyms and camel case, if it is the first word \\nwithin a variable name it should be lower case, if it is not the first \\nword, it should be all upper case. E.G:  \\no jiraTaskDescription (and not JIRATaskDescription)  \\no accountIBAN (and not accountIban)  \\n4. When using acronyms and proper case,  every letter of the \\nacronym should be upper case, e.g.  \\no case class ETLConfig(hdfsPath: String)  \\n5. Packages all lower case, including package objects.  \\n \\nAbbreviation \\n(don't do it!)  Dont abbreviate words in names, for example:  \\n• accountDate (not accountDt)  \\n• accountDateTime (not accountDtTm, and not accountDatetime)  \\nExceptions  \\n• When using Id (for Identification or Identifier), do not shorten \\nand use as lowerCamelCase, for example accountId  \\nSplit long \\nlines of code  Long lines of code should be split over multiple lines. The Scala style \\nguide suggests 80 characters as an upper bound, but  discretion is \\nallowed (R&D use 120 because they have big monitors!). If it doesn't fit \\non your screen wit hout scrolling then  the line definitely needs splitting!\"),\n",
       " Document(metadata={'source': '../data/guidlines.pdf', 'page': 1}, page_content='General \\nScala/Spark   \\nComments  If you need to comment your code to explain something, instead \\nquestion whether you can write the code in a clearer way (generally the \\nway to do this should result in more functional code).  Good code should \\nread like a story; if more detail is required, the developer can click \\nthrough to see more information. A simple example code which follows \\nthis notion is as follows:  \\n \\nExamples of where comments are no rmally required:  \\n1. Where decisions are being made which can\\'t be explained \\nthrough code (e.g. features of the data and business feedback \\nwhich determined how to write a score)  \\n2. \"TODO\"s and \"FIXME\" comments (see below)  \\nScaladocs  See style guide. Scaladocs are  effectively comments about what \\nsomething does and for methods/functions the parameters it takes. \\nObjects and key methods/functions should have Scaladocs explaining \\nwhat they are/do. An example from the style guide:  \\n1. /** Creates a person with a given name and birthdate  \\n2. * \\n3. * @param name their name  \\n4. * @param birthDate the person\\'s birthdate  \\n5. * @return a new Person instance with the age determined by the  \\n6. * birthdate and current date.  \\n7. */ \\n8. def apply (name : String , birthDate : java.util.Date ) = {} \\n9. }'),\n",
       " Document(metadata={'source': '../data/guidlines.pdf', 'page': 2}, page_content='General \\nScala/Spark   \\nTODO/FIXME  When something is left to do, use a TODO:  \\n//TODO: Add support for Longs  \\nWhen something may not work as expected, use a FIXME:  \\n//FIXME: Pattern match will fail if user provides Integer parameters  \\nIn both cases, be  specific  about what the task is,  as it may be a different \\ndeveloper who works on the task.  \\nHard -coded \\nconfiguration  Do not hard code configuration! Examples which should be externalised \\ninto a configuration project (e.g. type -safe) include:  \\n1. Locations of raw data / output data  \\n2. Iteration number  \\n3. Dates   \\n4. Scoring Parameters where the client wishes to be able to change \\nthese parameters  \\nRegular \\nExpressions  Regular expressions are compiled at run time, and add significant \\noverhead. As such, where possible they should be defined once on ly. In \\nother words:  \\n \\n \\nSpark only   \\nBroadcast \\njoins  When one dataset in a join is small it is a good idea to suggest to Spark  \\nto broadcast  the small dataset.  \\nlargeDF .join(broadcast (smallDF ), Seq(\"foo\" )) \\nA broadcast joi n is sort of like a hash merge in SAS - instead of shuffling  \\nboth datasets  to do a join the smaller one is \\'sent\\' to each executor  \\nwhich means the large dataset  doesn\\'t need to \\'move\\'. This is more  \\nefficient.'),\n",
       " Document(metadata={'source': '../data/guidlines.pdf', 'page': 3}, page_content='Joins on same \\ncolumn(s)  Use df1.join(df2, Seq( \"id\"))  \\nDont Use  df1.join(df2, df1(\"id\") === df2(\"id\"))   \\nThis is only applicable where you are joining two tables on a column (or  \\ncolumns) with the same name.   This avoids ending up with duplicate  \\ncolumns in the output dataframe and is much more succinct, especially  \\nwhen joining on multiple columns  \\nPrefer selec t \\nover drop  Instead of doing a long series of drops  \\ne.g. df.drop(\"a\").drop(\"b\").drop(\"c\").drop(\"d\").drop(\"e\")  \\nconsider selecting instead  \\ne.g. df.select(\"g\",\"h\",\"i\",\"j\",\"k\")  \\nSelecting makes it clear what is on the output dataframe. Furthermore, \\nSpark will n ot complain if you try and drop a column which doesn\\'t exist \\n(which in general is not desirable).  \\nAdditional benefits of select over drop include being able to choose the \\norder of the columns and also rename them in the same step: \\ndf.select($\"g\".as(\"moreLo gicalName\"),...)  \\nColumn \\nreferencing  There are 3 ways of referencing columns:  \\n• col(\"columnName\")  \\n• $\"columnName\"  \\n• \\'columnName  \\n$\"columnName\" is preferred as it is short, but won\\'t ruin code \\nhighlighting if you have code in a text editor such as notepad++. For t his \\nreason, avoid  \\'columnName  \\nNote that it is very common to do some coding in notepad++ on projects \\nwhen doing ad -hoc coding work before moving it into an etl/scoring \\nproject.  \\nExternal comprehensive coding guidelines:  \\nhttp://docs.scala -lang.org/style/  \\nhttps://github.com/alexandru/scala -best -practices'),\n",
       " Document(metadata={'source': '../data/guidlines.pdf', 'page': 4}, page_content='http://www.lihaoyi.com/post/StrategicScalaStylePracticalTypeSafety.html  \\nhttps://pavelfatin.com/scala -collections -tips-and-tricks/  \\nhttps://github.com/alexandru/scala -best -practices  \\nhttps://twitter.github.io/scala_school/')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfLoader=PyPDFLoader(file_path='../data/guidlines.pdf')\n",
    "pages = pdfLoader.load_and_split()\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract tables from pdf and convert into pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           General\\nScala/Spark  \\\n",
      "0                    Camel Case   \n",
      "1  Abbreviation\\n(don't do it!)   \n",
      "2     Split long\\nlines of code   \n",
      "\n",
      "                                                      \n",
      "0  Use camel casing, not underscores! Vals/vars/m...  \n",
      "1  Dont abbreviate words in names, for example:\\n...  \n",
      "2  Long lines of code should be split over multip...  \n",
      "  General\\nScala/Spark                                                   \n",
      "0             Comments  If you need to comment your code to explain so...\n",
      "1            Scaladocs  See style guide. Scaladocs are effectively com...\n",
      "        General\\nScala/Spark  \\\n",
      "0                 TODO/FIXME   \n",
      "1  Hard-coded\\nconfiguration   \n",
      "2       Regular\\nExpressions   \n",
      "\n",
      "                                                      \n",
      "0  When something is left to do, use a TODO:\\n//T...  \n",
      "1  Do not hard code configuration! Examples which...  \n",
      "2  Regular expressions are compiled at run time, ...  \n",
      "   Joins on same\\ncolumn(s)  \\\n",
      "0  Prefer select\\nover drop   \n",
      "1       Column\\nreferencing   \n",
      "\n",
      "  Use df1.join(df2, Seq(\"id\"))\\nDont Use df1.join(df2, df1(\"id\") === df2(\"id\"))\\nThis is only applicable where you are joining two tables on a column (or\\ncolumns) with the same name. This avoids ending up with duplicate\\ncolumns in the output dataframe and is much more succinct, especially\\nwhen joining on multiple columns  \n",
      "0  Instead of doing a long series of drops\\ne.g. ...                                                                                                                                                                                                                                                                                   \n",
      "1  There are 3 ways of referencing columns:\\n• co...                                                                                                                                                                                                                                                                                   \n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "pdf_path = '../data/guidlines.pdf'\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for pages in pdf.pages:\n",
    "        table = pages.extract_table()\n",
    "        #print(table)\n",
    "        if table:\n",
    "            df = pd.DataFrame(table[1:], columns=table[0])\n",
    "            print(df)\n",
    "\n",
    "#print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
